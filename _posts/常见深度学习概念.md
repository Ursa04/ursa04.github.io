# 一些DeepLearning的概念

---

## 卷积(Convolution)

用 “卷积核”（小矩阵）滑动遍历输入数据，逐元素相乘后求和得到输出。

主要用于卷积神经网络（CNN），是图像分类、目标检测、语义分割等任务的基础，也可用于语音信号处理、时序数据建模。

<center>

![convolution](image/Convolution.png)
</center>

1. 将卷积核 (小矩阵) 放在输入数据上
2. 对应元素相乘后求和，得到输出特征图的一个像素
3. 卷积核按指定步长滑动，重复计算直至覆盖整个输入

**符号：**
- i：输入尺寸
- o：输出尺寸
- k：卷积核大小(kernel size)
- s：步幅(stride)
- p：边界扩充(padding)

因此有：
$$
o=\frac{i-k+2p}{s} + 1
$$

---

## 反卷积/上采样

通过特定的卷积操作放大特征图尺寸，恢复空间分辨率

### 上采样算法

1. **双线性插值**
   适用于需要在已知的四个顶点值之间进行插值的情况，且不需要学习额外的参数
   <center>

   ![bilinear](https://i-blog.csdnimg.cn/direct/20409a93735a4819a8ed79cfded2fcd1.png)
   </center>

   $$
   R_1 = Q_{11}\frac{x_2-x}{x_2-x_1} + Q_{21}\frac{x-x_1}{x_2-x_1} \\
   R_2 = Q_{12}\frac{x_2-x}{x_2-x_1} + Q_{22}\frac{x-x_1}{x_2-x_1} \\ 
   P = R_1\frac{y_2-y}{y_2-y_1} + R_2\frac{y-y_1}{y_2-y_1}
   $$

2. **转置卷积（反卷积）**
   <center> 
   
   ![ConvTransposed1](image/ConvTranspose1.png)
   </center>
   转置卷积过程中stride始终为1

   1. 观察output如何通过卷积形成input，确定s、k、p
   2. input四周填充行/列数：k-p-1
   3. input像素间填充行/列数：s-1

   pytorch中，转置卷积的参数就是output卷积形成input的参数，比如图中最左侧的代码为：
   ```python
   torch.nn.ConvTranspose2d(in_channels=1,
                            out_channels=1,
                            kernel_seze=1,
                            stride=1,
                            padding=0)
   ```

   下图可以和卷积做对比，发现卷积核经过了转置，做到了$2*2\rightarrow3*3$
   <center> 
   
   ![ConvTransposed2](image/ConvTranspose2.png)
   </center>

   **转置卷积的卷积核并不是将下采样的卷积核简单的进行转置得到，而是同样要经过训练来确定卷积核中的参数**

---

## 卷积核 (Kernel)

一个小尺寸矩阵 (常见 3×3、5×5)，是滤波器中的核心权重矩阵

---

## 特征图 (Feature Map)

卷积操作的输出结果，是输入数据的特征表示，具有以下特点：

- 尺寸通常小于输入 (取决于步长和填充)
- **通道数**等于卷积核数量 (每种卷积核生成一个通道)
- 浅层网络提取边缘、颜色等低级特征
- 深层网络提取纹理、形状等高级语义特征

---

## 步长 (Stride)

卷积核在输入数据上每次滑动的像素距离

**降采样**：步长>1 ，卷积核跳跃移动，使输出尺寸显著减小

输出尺寸 = floor ((输入尺寸 - 卷积核尺寸 + 2× 填充)/ 步长) + 1

---

## 填充 (Padding)

在输入数据边缘添加额外像素 (通常为 0)，使卷积操作能完整覆盖边缘区域，分为：

- same 填充：添加填充使输出尺寸与输入相同 (保持空间信息)
- valid 填充：不添加填充，输出尺寸会减小

---

## 池化 (Pooling)

对特征图进行降采样，减小空间尺寸的操作

可以减少特征图尺寸、降低计算量，防止过拟合，且池化不改变通道数

### 最大池化 (Max Pooling)

在池化窗口 (如 2×2 区域) 中取最大值作为输出
作用：保留显著特征，抑制噪声，对微小位移保持鲁棒性

### 平均池化 (Average Pooling)

取池化窗口内所有元素的平均值作为输出
作用：平滑特征，保留整体统计信息

---

## 激活函数（Activation Function）

激活函数是神经网络能够学习非线性特征的根本原因

### 常用的激活函数

1. ReLU
   $$ f(x) = max(0, x) $$
2. Sigmoid - S 
   $$ f(x) = \frac{1}{(1 + e^{-x})} $$

---

## 图像分割（Image Segmentation）

将图像从 “整体” 拆分为多个有意义的 “局部区域” 的过程，不关注区域类别，只关注 “分割边界” 和 “区域完整性”。

### 分类

- 语义分割：对图像中每一个像素进行分类，赋予其明确的语义标签（如天空、汽车、行人、草地），最终输出与输入图像尺寸相同的 “语义标签图”。
- 实例分割：区分同一类别的不同个体（如区分两只猫）
- 全景分割（语义 + 实例）：同时标注类别和个体，覆盖图像所有像素。

---

## 感受野

感受野是输出特征图上的一个像素，对应原始输入图像（或前一层特征图）的局部区域大小。

---

## 量化

量化是将神经网络中高精度数值(如32位浮点数)转换为低精度表示(如8位整数)的过程，通过牺牲少量精度换取计算效率的大幅提升。

---

## 损失函数

损失函数（Loss Function）是深度学习中**量化模型预测值与真实标签之间误差**的核心函数。

### 回归任务

回归任务的核心是预测**连续型数值**（如房价、温度、销量、目标检测的坐标偏移）

1. **均方误差（又称MSE、L2）**
   最经典的回归损失
   - 公式：$L_{MSE}=\frac{1}{n}\sum_{i=1}^n(\hat{y}_i - y_i)^2$（$n$为样本数）
   - 特点：平方放大异常值的误差
   - 适用场景：无异常值、对精度要求高的任务

2. **平均绝对误差（MAE，L1损失）**
   抗异常值的回归损失
   - 公式：$L_{MAE}=\frac{1}{n}\sum_{i=1}^n|\hat{y}_i - y_i|$
   - 特点：对异常值鲁棒性较高
   - 适用场景：含较多异常值的回归任务

### 分类任务

字面意思，用来给样本做分类的任务

1. **二元交叉熵损失（BCE、对数损失）**
  二分类任务的标配损失函数
   - 公式：$L_{BCE}=-\frac{1}{n}\sum_{i=1}^n\left[y_i\log\hat{y}_i + (1-y_i)\log(1-\hat{y}_i)\right]$
  （$\hat{y}_i\in[0,1]$为样本$i$属于正类的概率，$y_i\in\{0,1\}$）
   - 特点：对预测错误的样本惩罚更严重；为避免log0无定义，工程中常加小数
   - 适用场景：所有二分类任务

2. **多元交叉熵损失（CE）**
   独热标签的多分类标配
   - 公式： $L_{CE}=-\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^C y_{i,j}\log\hat{y}_{i,j}$
   （$C$为类别数，$y_{i,j}\in\{0,1\}$是样本$i$在类别$j$的真实标签（独热），$\hat{y}_{i,j}$是样本$i$属于类别$j$的预测概率）
   - 特点：仅计算
  


# 深度学习中的损失函数：定义、分类与常见例子
损失函数（Loss Function）是深度学习中**量化模型预测值与真实标签之间误差**的核心函数，也是模型参数优化的**唯一目标**——训练模型的本质就是通过梯度下降等优化算法不断最小化损失函数的值，让模型的预测结果尽可能贴近真实情况。

### 核心基础概念
1. 数学形式：通常记为$L(y, \hat{y})$，其中$y$是样本的**真实标签**，$\hat{y}$是模型的**预测值**，输出为**标量**（保证反向传播时梯度可计算）。
2. 损失函数vs代价函数：**损失函数**针对**单个样本**的误差计算，**代价函数（Cost Function）** 是整个训练集的损失均值（也叫全局损失），工程中常将二者混用。
3. 核心要求：除少数特例（如合页损失），损失函数需**可微（或次可微）**，否则无法通过反向传播计算梯度、更新参数。

## 一、损失函数的核心分类
深度学习中损失函数的分类**以任务类型为核心依据**（最直观、最实用），因为不同任务的标签形式、预测目标差异极大，损失函数的设计需完全匹配任务需求。

主流分类为三大类：**回归任务损失函数**、**分类任务损失函数**（二分类/多分类），此外还有针对**度量学习、生成任务、序列任务**的特殊损失函数（补充常见类型）。

以下分类均为**工程中最常用、最经典**的类型，剔除小众且少用的损失函数，聚焦入门与实际应用。

## 二、各类别下的常见损失函数
### （一）回归任务损失函数
回归任务的核心是预测**连续型数值**（如房价、温度、销量、目标检测的坐标偏移），标签$y$是实数，模型预测值$\hat{y}$也为实数。
#### 1. 均方误差（MSE，L2损失）
**最经典的回归损失，入门首选**
- 公式：$L_{MSE}=\frac{1}{n}\sum_{i=1}^n(\hat{y}_i - y_i)^2$（$n$为样本数）
- 特点：
  ① 对误差做**平方运算**，会**放大异常值的误差**（异常值对模型影响大）；
  ② 处处可微，梯度为$\frac{2}{n}\sum_{i=1}^n(\hat{y}_i - y_i)$，**梯度随误差减小而减小**，训练后期收敛更稳定；
  ③ 凸函数，易找到全局最优解。
- 适用场景：无异常值、对预测精度要求高的常规回归任务（如房价预测、气温预测）。

#### 2. 平均绝对误差（MAE，L1损失）
**抗异常值的回归损失**
- 公式：$L_{MAE}=\frac{1}{n}\sum_{i=1}^n|\hat{y}_i - y_i|$
- 特点：
  ① 对误差做**绝对值运算**，**对异常值鲁棒**（不会放大异常值误差）；
  ② 梯度恒定为$\pm\frac{1}{n}$，误差大时参数更新速度稳定，不易陷入局部最优；
  ③ **在$\hat{y}_i=y_i$处不可导**（零点梯度无定义），工程中用**次梯度**解决。
- 适用场景：含较多异常值的回归任务（如销售额预测、医疗指标回归）。

#### 3. 平滑L1损失（Huber Loss，鲁棒L1损失）
**MSE与MAE的结合体，工业界主流**
- 公式：$L_{Huber}=\begin{cases}
\frac{1}{2\delta}(\hat{y}-y)^2, & |\hat{y}-y| \leq \delta \\
|\hat{y}-y| - \frac{\delta}{2}, & |\hat{y}-y| > \delta
\end{cases}$
  （$\delta$为阈值，通常取1，可自定义）
- 特点：
  ① **误差小时用MSE**（保持可导、收敛快的优势）；**误差大时用MAE**（保持抗异常值的优势）；
  ② 处处可微，解决了MAE的零点不可导问题。
- 适用场景：**目标检测、关键点检测**等计算机视觉回归任务（如Faster R-CNN、YOLO中预测边界框坐标，是工业界回归任务的首选）。

### （二）二分类任务损失函数
二分类任务的核心是预测样本属于**两个类别之一**（如正/负、是/否、患病/健康），真实标签$y$通常取$\{0,1\}$，模型最后一层需用**Sigmoid激活函数**将预测值映射到$[0,1]$区间（表示属于正类的概率）。

#### 1. 二元交叉熵损失（BCE，对数损失）
**二分类任务的标配损失函数**
- 公式：$L_{BCE}=-\frac{1}{n}\sum_{i=1}^n\left[y_i\log\hat{y}_i + (1-y_i)\log(1-\hat{y}_i)\right]$
  （$\hat{y}_i\in[0,1]$为样本$i$属于正类的概率，$y_i\in\{0,1\}$）
- 特点：
  ① 对**预测错误的样本惩罚更重**（如真实值为1，预测值为0.1时，$\log0.1\approx-2.3$，损失远大于轻微错误）；
  ② 梯度更新有效，不会出现梯度消失（只要预测值未完全偏离0/1）；
  ③ 若模型预测值为0或1（过拟合/激活函数饱和），会出现$\log0$无定义，工程中会加**小常数$\epsilon$**（如$\log(\hat{y}_i+\epsilon)$）避免。
- 适用场景：所有二分类任务（如垃圾邮件检测、人脸检测、情感二分类）。

#### 2. 合页损失（Hinge Loss）
**最大间隔分类损失，经典但深度学习中少用**
- 公式：$L_{Hinge}=\frac{1}{n}\sum_{i=1}^n\max(0, 1 - y_i\hat{y}_i)$
  （注意：此处$y_i\in\{-1,1\}$，$\hat{y}_i$是模型**未经过Sigmoid**的原始输出）
- 特点：
  ① 核心思想是**最大间隔**：仅惩罚**分类错误**和**置信度低的正确样本**，对置信度高的正确样本损失为0；
  ② 对异常值鲁棒，非凸函数（深度学习中收敛慢）；
  ③ 是支持向量机（SVM）的核心损失函数。
- 适用场景：对分类间隔有要求的二分类任务，深度学习中极少单独使用，多与其他损失结合。

### （三）多分类任务损失函数
多分类任务的核心是预测样本属于**多个类别中的一个**（如图像分类的猫/狗/鸟、文本分类的体育/财经/娱乐），真实标签常见两种形式：**独热编码（One-Hot）**（如3类任务中猫为$[1,0,0]$）或**整数编码**（如猫为0、狗为1、鸟为2），模型最后一层需用**Softmax激活函数**将预测值映射为**类别概率分布**（所有类别概率和为1）。

#### 1. 多元交叉熵损失（CE，Categorical Cross-Entropy）
**独热标签的多分类标配**
- 公式：$L_{CE}=-\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^C y_{i,j}\log\hat{y}_{i,j}$
  （$C$为类别数，$y_{i,j}\in\{0,1\}$是样本$i$在类别$j$的真实标签（独热），$\hat{y}_{i,j}$是样本$i$属于类别$j$的预测概率）
- 特点：
  ① 仅计算**真实类别对应的预测概率**的对数损失（其他类别$y_{i,j}=0$，贡献为0），计算高效；
  ② 与Softmax激活函数强绑定，是多分类任务的基础损失。
- 适用场景：真实标签为**独热编码**的多分类任务（如小样本图像分类、短文本分类）。

#### 2. 稀疏多元交叉熵损失（Sparse Categorical Cross-Entropy）
**大数据集多分类的首选**
- 公式：$L_{Sparse-CE}=-\frac{1}{n}\sum_{i=1}^n\log\hat{y}_{i,y_i}$
  （$y_i\in\{0,1,2,...,C-1\}$是样本$i$的真实类别整数编码，$\hat{y}_{i,y_i}$是样本$i$属于真实类别的预测概率）
- 特点：
  ① 无需对真实标签做独热编码，**大幅节省内存**（尤其当类别数$C$很大时，如ImageNet有1000类，独热编码会产生大量0）；
  ② 计算逻辑与多元交叉熵完全一致，仅标签输入形式不同。
- 适用场景：真实标签为**整数编码**的多分类任务（如大规模图像分类、语音识别、自然语言理解）。

#### 3. KL散度损失（相对熵损失）
**衡量概率分布差距的损失，生成任务常用**
- 公式：$D_{KL}(P||Q)=-\sum_{x}P(x)\log Q(x) + \sum_{x}P(x)\log P(x)$
  （$P$是真实概率分布，$Q$是模型预测的概率分布）
- 特点：
  ① KL散度表示**用分布Q近似分布P的信息损失**，值越小表示两个分布越接近；
  ② 当$P$是独热分布时，$\sum_{x}P(x)\log P(x)=0$，此时$D_{KL}(P||Q)=$多元交叉熵损失，即**最小化KL散度等价于最小化交叉熵**。
- 适用场景：生成模型（如VAE、GAN的部分变体）、概率模型的多分类任务，也可用于模型蒸馏。

### （四）其他常用特殊损失函数
针对**度量学习、序列任务**的经典损失，虽非基础分类/回归损失，但工程中高频使用：
1. **对比损失（Contrastive Loss）**：用于**相似度学习**（如人脸识别、推荐系统、图像检索），核心是让**同类样本的特征距离尽可能近，异类样本的特征距离尽可能远**。
2. **CTC损失（连接主义时间分类损失）**：用于**序列标注/语音识别**（如语音转文字、手写体识别），解决**输入序列与输出序列长度不匹配**的问题。
3. **Dice损失**：用于**图像分割**（尤其医学图像分割的小目标/前景区域），解决样本不平衡问题，核心是计算预测掩码与真实掩码的**交并比（IoU）** 相关损失。

## 三、损失函数的选择小技巧
1. **匹配任务类型**：回归用MSE/平滑L1，二分类用BCE，多分类用CE/稀疏CE，是永远不会出错的基础选择；
2. **考虑异常值**：有异常值选MAE/平滑L1，无异常值选MSE；
3. **考虑标签形式**：多分类中大数据集/整数标签选稀疏CE，小数据集/独热标签选普通CE；
4. **考虑样本不平衡**：分类任务若正负样本/各类别样本占比差异大，需在基础损失上**加权重**（如带权重的BCE/CE），或用Focal Loss（交叉熵的改进版，解决样本不平衡）。

## 四、经典损失函数使用总结表
| 任务类型 | 常见损失函数 | 核心优势 | 适用场景 |
|----------|--------------|----------|----------|
| 回归     | 均方误差（MSE） | 可导、收敛快、凸函数 | 无异常值的常规回归 |
| 回归     | 平滑L1损失    | 抗异常值、处处可导 | 目标检测/关键点检测的回归 |
| 二分类   | 二元交叉熵（BCE） | 惩罚错误预测、梯度有效 | 所有二分类任务 |
| 多分类   | 稀疏多元交叉熵 | 节省内存、计算高效 | 大数据集/整数标签多分类 |
| 生成/概率模型 | KL散度损失 | 衡量概率分布差距 | VAE、模型蒸馏、概率多分类 |
